http {
    upstream backend_servers {
        least_conn;                     # Enable least connections load balancing

        # Add your Ollama servers here eg. if one of them was http://localhost:11434/api then you would add `server localhost:11434`
        server X.X.X.X:11435;          # Ollama server 1
        server X.X.X.X:11436;          # Ollama server 2
        server X.X.X.X:11437;          # Ollama server 3
        server X.X.X.X:11438;          # Ollama server 4
    }

    server {
        listen 9090;                     # This is the port you would use for the Ollama API URL in the WebUI eg. http://localhost:9090/api

        location / {
            proxy_pass http://backend_servers; # Forward requests to the upstream block
        }
    }
}
worker_processes 1;

events {
    worker_connections 1024;
}
